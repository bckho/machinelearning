{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Back propagation and gradient descent - updating weights and biases\n",
    "_Author: Maurice Snoeren_<br>\n",
    "This notebook explains back propagation and how the weights and biases are updated using gradient descent algorithm with the update rule. We will use the example that is used in an earlier notebook, which is given by the figure below.\n",
    "\n",
    "<img src=\"./images/ann1.png\" width=\"400px\" />\n",
    "\n",
    "Note that the nodes that are given have inputs, weights, biases and an activation function. This is shown by the figure below. It is important to understand this, because we will require all the forward propagation equations and functions.\n",
    "\n",
    "<img src=\"./images/perceptron2.png\" width=\"200px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Training of a neural network is finding the correct weights and biases, that results in the desired output based on a given input. We could try random number for the weights and biases. However, it will take too much time because of the many variables. For our simple example, the neural network already contains 20 weights and 6 biases. That are already 26 different variables to fit. We need to find a way to calculate which weights and biases need to be changed and result to the final solution.\n",
    "\n",
    "That is were gradient descent comes in. With the gradient descent algorithm we need to define the error of the neural network based on a known input and desired output. Another term for error is cost function, which is normally used within the literature. The cost function gives us an idea how bad (or how good) the solution is. It is based on a test sample $T$ that contains a predefined input $\\hat{x}$ and the belonging desired output $\\hat{y}$. We define this test set as $T = [\\hat{x}, \\hat{y}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "All the neural networks will randomly fill the weights and biases, to have some starting point. From this moment, we are able to train the network by using sample from the test set. In this example, we will use one sample to train the network. Generally this is a bad idea. For now we are only focussing on how a neural network can be trained. When we hava a sample $T = [\\hat{x}, \\hat{y}]$, we start by calculate the output of the neural network $y$, using the forward propagation equations, based on the input vector given by $\\hat{x}$. As summary, we will give these equations below:\n",
    "\n",
    "$z_h = \\hat{x} * W_{hx} + b_h$<br>\n",
    "$h   = f(z_h)$\n",
    "\n",
    "$z_y = h * W_{yh} + b_y$<br>\n",
    "$y   = f(z_y)$\n",
    "\n",
    "We already filled in the sample input $\\hat{x}$ to the network and find the output $y$. This output is based on the current weights and biases of the network. When the network is not trained yet, the output should not show the correct solution. The error or cost can be seen as the distance between the calculated output $y$ and the desired output $\\hat{y}$. (In the literature, the calculated output is the hypothesis). We are able to calculate these by a well-known quadratic cost function (there are many types of cost functions, we will stick to this one):\n",
    "\n",
    "$J = \\frac{1}{2}(y - \\hat{y})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
