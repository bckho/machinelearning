{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Python implementation ANN from scratch back propagation\n",
    "_Author: Maurice Snoeren_<br>\n",
    "This notebook adds backpropagationan to the Python code as given by notebook 3. \n",
    "<img src=\"./images/ann2.png\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not yet familiar with this model and with the code below, please take a look to notebook 3 first. We will add the backpropagation to the class ANN first. The theory convention and variable names will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ANN:\n",
    "    def __init__(self, num_input_nodes, num_output_nodes, output_activation): # construct the ANN object\n",
    "        self.num_input_nodes   = num_input_nodes   # Hold the number of input nodes\n",
    "        self.num_output_nodes  = num_output_nodes  # Hold the number of output nodes\n",
    "        self.output_activation = output_activation # Hold the number of input nodes\n",
    "        self.hidden_layers     = [] # Hold all the hidden_layer classes\n",
    "        self.Wy                = np.random.rand(num_input_nodes, num_output_nodes) # Hold output layer weight matrix\n",
    "        self.by                = np.zeros((1, num_output_nodes)) # Biases vector of the output nodes\n",
    "        self.x                 = [] # Hold the input vector that is used for calculation\n",
    "        self.zy                = [] # Hold the summation of the input and bias with the weights\n",
    "        self.y                 = [] # hold the output vector\n",
    "\n",
    "    def get_weight_matrix(self): # getter for the weight matrix\n",
    "        return self.Wy\n",
    "\n",
    "    def set_weight_matrix(self, Wy): # setter for the weight matrix\n",
    "        self.Wy = Wy\n",
    "\n",
    "    def get_biases_vector(self): # getter for the bias vector\n",
    "        return self.by\n",
    "\n",
    "    def set_biases_vector(self, by): # setter for the bias vector\n",
    "        self.by = by\n",
    "\n",
    "    def add_hidden_layer(self, hidden_layer): # add a new hidden layer to the ANN\n",
    "        self.hidden_layers.append(hidden_layer) # Add the HiddenLayer class to the array\n",
    "        self.Wy = np.random.rand(hidden_layer.num_hidden_nodes, self.num_output_nodes) # Re-initializes the output matrix\n",
    "                                                                                       # based on number of hidden nodes\n",
    "    def get_total_hidden_layers(self): # return how many hidden layers are configured\n",
    "        return len(self.hidden_layers)\n",
    "\n",
    "    def get_hidden_layer(self, i): # returns the hidden layer given the index (no checks performed!)\n",
    "        return self.hidden_layers[i]\n",
    "    \n",
    "    def forward_propagation(self, x):\n",
    "        self.x = x # store the input that we have used for the calculation\n",
    "\n",
    "        if ( len(self.hidden_layers) == 0): # Within our design it is possible that no hidden layers exist!\n",
    "            self.zy = np.dot( self.x, self.Wy ) + self.by\n",
    "            self.y = self.output_activation.forward( self.zy )\n",
    "\n",
    "        else: # when we have hidden layers, we iterate over these hidden layers\n",
    "            input_vector = self.x # this input_vector is used to pass to the next layer\n",
    "            for hidden_layer in self.hidden_layers:\n",
    "                output_vector = hidden_layer.forward_propagation(input_vector) # the hidden layer class calculates the \n",
    "                                                                               # output based on the input.\n",
    "                input_vector = output_vector # the next hidden layer will use the output of this hidden layer\n",
    "            \n",
    "            # calculate the output of the neural network using the output of the last hidden layer as input\n",
    "            self.zy = np.dot( input_vector, self.Wy ) + self.by # first calculate the weight and bias result\n",
    "            self.y  = self.output_activation.forward( self.zy ) # calculate the activation function\n",
    "\n",
    "        return self.y # return the output activation vector of all the nodes\n",
    "    \n",
    "    def cost_function(self, input_example, output_desired):\n",
    "        self.forward_propagation(input_example)  # Perform first the forward propagation calculation\n",
    "        J = 0.5 * ( self.y - output_desired )**2 # Calculate the cost function\n",
    "        return J\n",
    "\n",
    "    def back_propagation(self, input_example, output_desired):\n",
    "        J = self.cost_function(input_example, output_desired)\n",
    "        \n",
    "        # start back propagation of the network\n",
    "        dJ_dy = ( self.y - output_desired )\n",
    "        dy_dzy   = self.output_activation.derivative( self.zy )\n",
    "        \n",
    "        delta = np.multiply( dJ_dy, dy_dzy ) # required to back propagate through the network (part of the derivation that propagates back into the network)\n",
    "        weights = self.Wy # weight required for the next layer\n",
    "        dJ_dWh = [] # Hold the weight gradients of the hidden layers\n",
    "\n",
    "        if ( len(self.hidden_layers) == 0): # When we do not have any hidden layers, we only have one weight matrix\n",
    "            dJ_dWy = np.dot( self.x.transpose(), delta )\n",
    "\n",
    "        else: # Loop over the hidden layers from back to start\n",
    "            dzy_dWy = self.hidden_layers[ len(self.hidden_layers)-1 ].h\n",
    "            dJ_dWy = np.dot( dzy_dWy.transpose(), delta )  # calculate the gradient of Wy\n",
    "\n",
    "            for hidden_layer in reversed(self.hidden_layers): # loop the hidden layers from back to start (reversed!)\n",
    "                result  = hidden_layer.back_propagation(delta, weights) # calculate gradient of hidden layer Wh\n",
    "                delta   = result['delta'] # update delat for the next layer\n",
    "                weights = result['W'] # update the weights matrix for the next layer\n",
    "                dJ_dWh.append( result['dJ_dWh']) # append the gradient hidden weight matrix to the array\n",
    "\n",
    "        return {'dJ_dWh': list(reversed(dJ_dWh)), 'dJ_dWy': dJ_dWy } # return the back propagation result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add the back propagation to the hidden layer class, so we are able to fully calculate the gradients of the weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNHiddenLayer:\n",
    "    def __init__(self, num_input_nodes, num_hidden_nodes, activation):\n",
    "        self.num_input_nodes  = num_input_nodes # number of nodes of the previous layer used as input\n",
    "        self.num_hidden_nodes = num_hidden_nodes # number of hidden nodes of this layer to be used\n",
    "        self.activation       = activation # the activation function that should be used for all hidden nodes\n",
    "        self.x                = [] # Input vector of this hidden layer\n",
    "        self.Wh               = np.random.rand(num_input_nodes, num_hidden_nodes) # Hidden weight matrix\n",
    "        self.bh               = np.zeros((1, num_hidden_nodes)) # Biases vector of the hidden layer\n",
    "        self.zh               = [] # Hold the summation of the input and bias with the weights\n",
    "        self.h                = [] # Hold the output vector of this hidden layer\n",
    "\n",
    "    def get_weight_matrix(self): # getter for the weight matrix of the hidden layer\n",
    "        return self.Wh\n",
    "\n",
    "    def set_weight_matrix(self, Wh): # setter for the weight matrix of the hidden layer\n",
    "        self.Wh = Wh\n",
    "\n",
    "    def get_biases_vector(self): # getter for the biases vector of the hidden layer\n",
    "        return self.bh\n",
    "\n",
    "    def set_biases_vector(self, bh): # setter for the biases vector of the hidden layer\n",
    "        self.bh = bh\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        self.x = x # store the input that is used for the calculation\n",
    "        self.zh = np.dot(x, self.Wh) + self.bh # first calculate the weight and bias result\n",
    "        self.h  = self.activation.forward( self.zh ) # calculate the activation function\n",
    "        \n",
    "        return self.h # return the output activation vector of all the nodes\n",
    "    \n",
    "    def back_propagation(self, prev_delta, prev_W):\n",
    "        delta = np.dot( prev_delta, prev_W.transpose() ) * self.activation.derivative( self.zh ) # this is required for\n",
    "                                                                                                 # next layer\n",
    "        dz1_dWh = self.x # Activation from the previous layer!\n",
    "        dJ_dWh  = np.dot( dz1_dWh.transpose(), delta ) # calculate the gradient of the weight matrix \n",
    "\n",
    "        return { 'delta': delta, 'W': self.Wh, 'dJ_dWh': dJ_dWh } # return the result of the backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is going allright! We still mis the derivative of the activation function class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNSigmoidActivation:\n",
    "    def forward(self, input_vector):\n",
    "        return 1/(1 + np.exp(-input_vector))\n",
    "    \n",
    "    def derivative(self, input_vector):\n",
    "        return np.exp(-input_vector) / ((1 + np.exp(-input_vector))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is in place and we can now try the class and see whether the calculation is done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: [[0.99560742 0.99001787]]\n"
     ]
    }
   ],
   "source": [
    "sa  = ANNSigmoidActivation() # construct the Sigmoid activation function\n",
    "\n",
    "ann = ANN(4, 2, sa) # create an ANN with four input nodes and two output nodes. The output nodes get the sigmoid\n",
    "                    # activation function.\n",
    "    \n",
    "ann.add_hidden_layer(ANNHiddenLayer(4, 10, sa)) # add a hidden layer with ten nodes, the input is four due to the\n",
    "                                                  # total of number of input nodes x, defines by the ANN.\n",
    "    \n",
    "ann.add_hidden_layer(ANNHiddenLayer(10, 10, sa)) # create another hidden layer with ten node, the input is ten nodes\n",
    "                                                   # due to the input nodes of the previoud hidden layer of ten.\n",
    "\n",
    "x = np.array([[0.1, 0.1, 0.1, 0.1]]) # create an example input vector x\n",
    "\n",
    "print( \"output: \" + str(ann.forward_propagation(x)) ) # print the output of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9877dc55dbbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# perform back propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# update the weight matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-83943f863844>\u001b[0m in \u001b[0;36mback_propagation\u001b[0;34m(self, input_example, output_desired)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mback_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_desired\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_desired\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# start back propagation of the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-83943f863844>\u001b[0m in \u001b[0;36mcost_function\u001b[0;34m(self, output_desired)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_desired\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_example\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Perform first the forward propagation calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutput_desired\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;31m# Calculate the cost function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_example' is not defined"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 1, 1, 1]]) # input sample\n",
    "y = np.array([[1, 0]])       # desired output\n",
    "alpha = 1                    # learning rate\n",
    "\n",
    "for i in range(10):\n",
    "    result = ann.back_propagation(x, y) # perform back propagation\n",
    "\n",
    "    # update the weight matrices\n",
    "    ann.set_weight_matrix(ann.get_weight_matrix() - alpha*result['dJ_dWy']) # update output weight matrix Wy\n",
    "    for i in range(ann.get_total_hidden_layers()): # update weight matrices of the hidden layers\n",
    "        hl = ann.get_hidden_layer(i)\n",
    "        wm = result['dJ_dWh'][i]\n",
    "        hl.set_weight_matrix( hl.get_weight_matrix() - alpha*wm )\n",
    "\n",
    "result   = ann.forward_propagation(x)\n",
    "accuracy = np.mean((result - y)**2)\n",
    "\n",
    "print( \"Final result: \" + str(result) )\n",
    "print( \"Accuracy    : \" + str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
